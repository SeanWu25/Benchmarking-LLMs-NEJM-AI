# Benchmarking-LLMs-NEJM-AI

This repository contains the code for benchmarking Large Language Models (LLMs) in our paper titled "A Comparative Study of Open-Source Large Language Models, GPT-4, and Claude 2: Multiple-Choice Test Taking in Nephrology." Here, you will find folders dedicated to the following tasks:

1. **Inference on Nephrology Dataset:**
   - Inside this repository, you will find folders for running inference on open-sourced LLMs, including Falcon, Orca-Mini, Vicuna, Llama-70b, and Koala. These LLMs will be assessed on their performance using the Nephrology dataset.

2. **Model Benchmarking:**
   - The code in this repository allows you to benchmark the outputs of the models based on several metrics:
     - Accuracy
     - BLEU (Bilingual Evaluation Understudy)
     - WER (Word Error Rate)
     - Cosine Similarity
     - Utilizing regular expressions, you can analyze and compare the performance of the LLMs with these metrics.

3. **Error Margins and Confidence Intervals:**
   - You can also calculate error margins by employing a 95% confidence interval. This statistical analysis will provide insights into the reliability of the results obtained from the LLMs.

Feel free to explore the code and datasets within this repository to replicate and expand upon our research findings in the field of Nephrology using open-source Large Language Models.
